{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "359436d4",
   "metadata": {},
   "source": [
    "# DISEASE OUTBREAK PREDICTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f904a1",
   "metadata": {},
   "source": [
    "importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db4ac024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kedar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask\\dataframe\\_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 13.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kedar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.dataframe as dd # parallel processing for pandas (to read large datasets) fixed memory issue\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from datetime import datetime, timezone\n",
    "nltk.download('wordnet')\n",
    "import locationtagger\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e25c7ce",
   "metadata": {},
   "source": [
    "Reading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e7b7a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading comments dataset (do not have enough computational power and memory space to read the entire dataset)\n",
    "data_comments = pd.read_csv('C:\\\\Users\\\\kedar\\\\OneDrive\\\\Desktop\\\\ML mini project\\\\DiseaseOutbreak_Dataset\\\\the-reddit-covid-dataset-comments.csv', nrows=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3745df85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit.id</th>\n",
       "      <th>subreddit.name</th>\n",
       "      <th>subreddit.nsfw</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>permalink</th>\n",
       "      <th>body</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>comment</td>\n",
       "      <td>hi1vsag</td>\n",
       "      <td>2riyy</td>\n",
       "      <td>nova</td>\n",
       "      <td>False</td>\n",
       "      <td>1635206399</td>\n",
       "      <td>https://old.reddit.com/r/nova/comments/qfs53d/...</td>\n",
       "      <td>When you scheduled your booster with CVS does ...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comment</td>\n",
       "      <td>hi1vs7i</td>\n",
       "      <td>2qhov</td>\n",
       "      <td>vancouver</td>\n",
       "      <td>False</td>\n",
       "      <td>1635206397</td>\n",
       "      <td>https://old.reddit.com/r/vancouver/comments/qf...</td>\n",
       "      <td>Didn't stop prices there though. New Zealand a...</td>\n",
       "      <td>0.1887</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comment</td>\n",
       "      <td>hi1vs5n</td>\n",
       "      <td>2qwzb</td>\n",
       "      <td>pregnant</td>\n",
       "      <td>False</td>\n",
       "      <td>1635206397</td>\n",
       "      <td>https://old.reddit.com/r/pregnant/comments/qfs...</td>\n",
       "      <td>I’m just waiting until after pregnancy to get ...</td>\n",
       "      <td>0.6720</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comment</td>\n",
       "      <td>hi1vs5v</td>\n",
       "      <td>2qixm</td>\n",
       "      <td>startrek</td>\n",
       "      <td>False</td>\n",
       "      <td>1635206397</td>\n",
       "      <td>https://old.reddit.com/r/startrek/comments/qft...</td>\n",
       "      <td>*The first duty of every Starfleet officer is ...</td>\n",
       "      <td>0.9562</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comment</td>\n",
       "      <td>hi1vs0l</td>\n",
       "      <td>2qsf3</td>\n",
       "      <td>ontario</td>\n",
       "      <td>False</td>\n",
       "      <td>1635206395</td>\n",
       "      <td>https://old.reddit.com/r/ontario/comments/qfkj...</td>\n",
       "      <td>Compare BC to Ontario for COVID. It's even mor...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      type       id subreddit.id subreddit.name  subreddit.nsfw  created_utc  \\\n",
       "0  comment  hi1vsag        2riyy           nova           False   1635206399   \n",
       "1  comment  hi1vs7i        2qhov      vancouver           False   1635206397   \n",
       "2  comment  hi1vs5n        2qwzb       pregnant           False   1635206397   \n",
       "3  comment  hi1vs5v        2qixm       startrek           False   1635206397   \n",
       "4  comment  hi1vs0l        2qsf3        ontario           False   1635206395   \n",
       "\n",
       "                                           permalink  \\\n",
       "0  https://old.reddit.com/r/nova/comments/qfs53d/...   \n",
       "1  https://old.reddit.com/r/vancouver/comments/qf...   \n",
       "2  https://old.reddit.com/r/pregnant/comments/qfs...   \n",
       "3  https://old.reddit.com/r/startrek/comments/qft...   \n",
       "4  https://old.reddit.com/r/ontario/comments/qfkj...   \n",
       "\n",
       "                                                body  sentiment  score  \n",
       "0  When you scheduled your booster with CVS does ...     0.0000      2  \n",
       "1  Didn't stop prices there though. New Zealand a...     0.1887     32  \n",
       "2  I’m just waiting until after pregnancy to get ...     0.6720      1  \n",
       "3  *The first duty of every Starfleet officer is ...     0.9562      1  \n",
       "4  Compare BC to Ontario for COVID. It's even mor...     0.0000     -2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking whether the data loaded\n",
    "data_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fdf84a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading posts dataset\n",
    "data_posts = pd.read_csv('C:\\\\Users\\\\kedar\\\\OneDrive\\\\Desktop\\\\ML mini project\\\\DiseaseOutbreak_Dataset\\\\the-reddit-covid-dataset-posts.csv',nrows=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19e96af5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit.id</th>\n",
       "      <th>subreddit.name</th>\n",
       "      <th>subreddit.nsfw</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>permalink</th>\n",
       "      <th>domain</th>\n",
       "      <th>url</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>post</td>\n",
       "      <td>qftvfn</td>\n",
       "      <td>3h9d4</td>\n",
       "      <td>autonewspaper</td>\n",
       "      <td>False</td>\n",
       "      <td>1635206379</td>\n",
       "      <td>https://old.reddit.com/r/AutoNewspaper/comment...</td>\n",
       "      <td>washingtontimes.com</td>\n",
       "      <td>https://www.washingtontimes.com/news/2021/oct/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Politics] - Kay Ivey, Alabama governor, direc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>post</td>\n",
       "      <td>qftusg</td>\n",
       "      <td>2x4yx</td>\n",
       "      <td>coronavirus</td>\n",
       "      <td>False</td>\n",
       "      <td>1635206322</td>\n",
       "      <td>https://old.reddit.com/r/Coronavirus/comments/...</td>\n",
       "      <td>twitter.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>More Floridians dead from COVID than Vietnam W...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>post</td>\n",
       "      <td>qftugv</td>\n",
       "      <td>3h9d4</td>\n",
       "      <td>autonewspaper</td>\n",
       "      <td>False</td>\n",
       "      <td>1635206292</td>\n",
       "      <td>https://old.reddit.com/r/AutoNewspaper/comment...</td>\n",
       "      <td>nzherald.co.nz</td>\n",
       "      <td>https://www.nzherald.co.nz/nz/news/article.cfm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[NZ] - Covid-19 coronavirus Delta outbreak: La...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>post</td>\n",
       "      <td>qftuge</td>\n",
       "      <td>3h9d4</td>\n",
       "      <td>autonewspaper</td>\n",
       "      <td>False</td>\n",
       "      <td>1635206291</td>\n",
       "      <td>https://old.reddit.com/r/AutoNewspaper/comment...</td>\n",
       "      <td>nzherald.co.nz</td>\n",
       "      <td>https://www.nzherald.co.nz/nz/news/article.cfm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[NZ] - Covid 19 Delta outbreak: Man arrested a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>post</td>\n",
       "      <td>qftucd</td>\n",
       "      <td>4egz78</td>\n",
       "      <td>cointuta</td>\n",
       "      <td>False</td>\n",
       "      <td>1635206281</td>\n",
       "      <td>https://old.reddit.com/r/CoinTuta/comments/qft...</td>\n",
       "      <td>cointuta.com</td>\n",
       "      <td>https://www.cointuta.com/covid-19-more-likely-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Covid-19 more likely to cause neurological iss...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type      id subreddit.id subreddit.name  subreddit.nsfw  created_utc  \\\n",
       "0  post  qftvfn        3h9d4  autonewspaper           False   1635206379   \n",
       "1  post  qftusg        2x4yx    coronavirus           False   1635206322   \n",
       "2  post  qftugv        3h9d4  autonewspaper           False   1635206292   \n",
       "3  post  qftuge        3h9d4  autonewspaper           False   1635206291   \n",
       "4  post  qftucd       4egz78       cointuta           False   1635206281   \n",
       "\n",
       "                                           permalink               domain  \\\n",
       "0  https://old.reddit.com/r/AutoNewspaper/comment...  washingtontimes.com   \n",
       "1  https://old.reddit.com/r/Coronavirus/comments/...          twitter.com   \n",
       "2  https://old.reddit.com/r/AutoNewspaper/comment...       nzherald.co.nz   \n",
       "3  https://old.reddit.com/r/AutoNewspaper/comment...       nzherald.co.nz   \n",
       "4  https://old.reddit.com/r/CoinTuta/comments/qft...         cointuta.com   \n",
       "\n",
       "                                                 url   selftext  \\\n",
       "0  https://www.washingtontimes.com/news/2021/oct/...        NaN   \n",
       "1                                                NaN  [deleted]   \n",
       "2  https://www.nzherald.co.nz/nz/news/article.cfm...        NaN   \n",
       "3  https://www.nzherald.co.nz/nz/news/article.cfm...        NaN   \n",
       "4  https://www.cointuta.com/covid-19-more-likely-...        NaN   \n",
       "\n",
       "                                               title  score  \n",
       "0  [Politics] - Kay Ivey, Alabama governor, direc...      1  \n",
       "1  More Floridians dead from COVID than Vietnam W...      1  \n",
       "2  [NZ] - Covid-19 coronavirus Delta outbreak: La...      1  \n",
       "3  [NZ] - Covid 19 Delta outbreak: Man arrested a...      1  \n",
       "4  Covid-19 more likely to cause neurological iss...      1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking whetehr the dataset is loaded\n",
    "data_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31c62055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_reported</th>\n",
       "      <th>Country_code</th>\n",
       "      <th>Country</th>\n",
       "      <th>WHO_region</th>\n",
       "      <th>New_cases</th>\n",
       "      <th>Cumulative_cases</th>\n",
       "      <th>New_deaths</th>\n",
       "      <th>Cumulative_deaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>AF</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>EMRO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>AF</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>EMRO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>AF</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>EMRO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>AF</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>EMRO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-07</td>\n",
       "      <td>AF</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>EMRO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Date_reported Country_code      Country WHO_region  New_cases  \\\n",
       "0    2020-01-03           AF  Afghanistan       EMRO          0   \n",
       "1    2020-01-04           AF  Afghanistan       EMRO          0   \n",
       "2    2020-01-05           AF  Afghanistan       EMRO          0   \n",
       "3    2020-01-06           AF  Afghanistan       EMRO          0   \n",
       "4    2020-01-07           AF  Afghanistan       EMRO          0   \n",
       "\n",
       "   Cumulative_cases  New_deaths  Cumulative_deaths  \n",
       "0                 0           0                  0  \n",
       "1                 0           0                  0  \n",
       "2                 0           0                  0  \n",
       "3                 0           0                  0  \n",
       "4                 0           0                  0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_stats = pd.read_csv('WHO-COVID-19-global-data.csv')\n",
    "data_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9711e4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in column of Comments dataset:  Date_reported           0\n",
      "Country_code         1407\n",
      "Country                 0\n",
      "WHO_region              0\n",
      "New_cases               0\n",
      "Cumulative_cases        0\n",
      "New_deaths              0\n",
      "Cumulative_deaths       0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_reported</th>\n",
       "      <th>Country</th>\n",
       "      <th>WHO_region</th>\n",
       "      <th>New_cases</th>\n",
       "      <th>Cumulative_cases</th>\n",
       "      <th>New_deaths</th>\n",
       "      <th>Cumulative_deaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>EMRO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>EMRO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>EMRO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>EMRO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-07</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>EMRO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Date_reported      Country WHO_region  New_cases  Cumulative_cases  \\\n",
       "0    2020-01-03  Afghanistan       EMRO          0                 0   \n",
       "1    2020-01-04  Afghanistan       EMRO          0                 0   \n",
       "2    2020-01-05  Afghanistan       EMRO          0                 0   \n",
       "3    2020-01-06  Afghanistan       EMRO          0                 0   \n",
       "4    2020-01-07  Afghanistan       EMRO          0                 0   \n",
       "\n",
       "   New_deaths  Cumulative_deaths  \n",
       "0           0                  0  \n",
       "1           0                  0  \n",
       "2           0                  0  \n",
       "3           0                  0  \n",
       "4           0                  0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_stats = data_stats.isna().sum()\n",
    "print('Nan values in column of Comments dataset: ',nan_stats)\n",
    "data_stats = data_stats.drop(columns = 'Country_code')\n",
    "data_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8d9bd0",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8675247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan values in column of Comments dataset:  type                  0\n",
      "id                    0\n",
      "subreddit.id          0\n",
      "subreddit.name        0\n",
      "subreddit.nsfw        0\n",
      "created_utc           0\n",
      "permalink             0\n",
      "body                  0\n",
      "sentiment         37937\n",
      "score                 0\n",
      "dtype: int64\n",
      "Nan values in each column of Posts dataset:  type                   0\n",
      "id                     0\n",
      "subreddit.id           0\n",
      "subreddit.name         0\n",
      "subreddit.nsfw         0\n",
      "created_utc            0\n",
      "permalink              0\n",
      "domain                 4\n",
      "url               257459\n",
      "selftext          742541\n",
      "title                  0\n",
      "score                  0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# analysing missing values\n",
    "nan_comments = data_comments.isna().sum()\n",
    "nan_posts = data_posts.isna().sum()\n",
    "print('Nan values in column of Comments dataset: ',nan_comments)\n",
    "print('Nan values in each column of Posts dataset: ',nan_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b63f9353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lots of missing values in comments dataset - sentiment column\n",
    "# lots of missing values in posts dataset - url and selftext columns\n",
    "# dropping these columns\n",
    "data_comments = data_comments.drop(columns = 'sentiment')\n",
    "data_posts = data_posts.drop(columns = ['url','selftext'])\n",
    "\n",
    "#dropping type,id, subreddit.id, subreddit.name,subreddit.nsfw, permalink in comments dataset since they aren't relevant to the prediction of an outbreak\n",
    "# similarly dropping type,id,subreddit.id, subreddit.name,subreddit.nsfw,permalink,domain\n",
    "data_comments = data_comments.drop(columns = ['type','id','subreddit.id', 'subreddit.name','subreddit.nsfw', 'permalink'])\n",
    "data_posts = data_posts.drop(columns = ['type','id','subreddit.id', 'subreddit.name','subreddit.nsfw','permalink','domain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98700ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in comments dataset:  Index(['created_utc', 'body', 'score'], dtype='object')\n",
      "Columns in posts dataset:  Index(['created_utc', 'title', 'score'], dtype='object')\n",
      "Nan values in column of Comments dataset:  created_utc    0\n",
      "body           0\n",
      "score          0\n",
      "dtype: int64\n",
      "Nan values in each column of Posts dataset:  created_utc    0\n",
      "title          0\n",
      "score          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# checking if chenges were made\n",
    "print('Columns in comments dataset: ',data_comments.columns)\n",
    "print('Columns in posts dataset: ',data_posts.columns)\n",
    "\n",
    "# re-analysing missing values\n",
    "nan_comments = data_comments.isna().sum()\n",
    "nan_posts = data_posts.isna().sum()\n",
    "print('Nan values in column of Comments dataset: ',nan_comments)\n",
    "print('Nan values in each column of Posts dataset: ',nan_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1798f9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting all utc objects to date-time objects\n",
    "data_comments['created_utc'] = pd.to_datetime(data_comments['created_utc'], unit='s', utc=True)\n",
    "data_posts['created_utc'] = pd.to_datetime(data_posts['created_utc'], unit='s', utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "539f43e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datatype of created utc column in comments dataset:  datetime64[ns, UTC]\n",
      "datatype of created utc column in posts dataset:  datetime64[ns, UTC]\n"
     ]
    }
   ],
   "source": [
    "# checking \n",
    "print('datatype of created utc column in comments dataset: ',data_comments['created_utc'].dtype)\n",
    "print('datatype of created utc column in posts dataset: ',data_posts['created_utc'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d6a6681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kedar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kedar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0723723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = ''.join([char for char in text if char.isalnum() or char.isspace()])\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Stemming (optional)\n",
    "    # tokens = [ps.stem(word) for word in tokens]\n",
    "    # Join the tokens back into a string\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "data_comments['body'] = data_comments['body'].apply(preprocess_text)\n",
    "data_posts['title'] = data_posts['title'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8054e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_locations(text):\n",
    "#     # Check if the input text is not NaN\n",
    "#     if not pd.isna(text):\n",
    "#         place_entity = locationtagger.find_locations(text=text)\n",
    "        \n",
    "#         # Extract countries, regions, and cities as strings\n",
    "#         countries = ', '.join(place_entity.countries) if place_entity.countries else ''\n",
    "# #         regions = ', '.join(place_entity.regions) if place_entity.regions else ''\n",
    "#         cities = ', '.join(place_entity.cities) if place_entity.cities else ''\n",
    "\n",
    "#         # Combine countries, regions, and cities into a single string\n",
    "#         result = f'{countries}, {regions}, {cities}'.strip(', ')\n",
    "\n",
    "#         # Print statements for checking\n",
    "#         print(f\"Input: {text}\")\n",
    "#         print(f\"Countries: {countries}\")\n",
    "#         print(f\"Regions: {regions}\")\n",
    "#         print(f\"Cities: {cities}\")\n",
    "#         print(f\"Result: {result}\")\n",
    "\n",
    "#         return result\n",
    "\n",
    "#     return 'unknown location'\n",
    "\n",
    "# # Apply the function to your single column and create a new 'locations' column\n",
    "# data_comments['locations'] = data_comments['body'].apply(extract_locations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65c39711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-10-25 23:59:39+00:00</td>\n",
       "      <td>politics kay ivey alabama governor directs age...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-10-25 23:58:42+00:00</td>\n",
       "      <td>floridians dead covid vietnam war video</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-10-25 23:58:12+00:00</td>\n",
       "      <td>nz covid19 coronavirus delta outbreak latest c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-10-25 23:58:11+00:00</td>\n",
       "      <td>nz covid 19 delta outbreak man arrested trying...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-10-25 23:58:01+00:00</td>\n",
       "      <td>covid19 likely cause neurological issues vacci...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                created_utc  \\\n",
       "0 2021-10-25 23:59:39+00:00   \n",
       "1 2021-10-25 23:58:42+00:00   \n",
       "2 2021-10-25 23:58:12+00:00   \n",
       "3 2021-10-25 23:58:11+00:00   \n",
       "4 2021-10-25 23:58:01+00:00   \n",
       "\n",
       "                                               title  score  \n",
       "0  politics kay ivey alabama governor directs age...      1  \n",
       "1            floridians dead covid vietnam war video      1  \n",
       "2  nz covid19 coronavirus delta outbreak latest c...      1  \n",
       "3  nz covid 19 delta outbreak man arrested trying...      1  \n",
       "4  covid19 likely cause neurological issues vacci...      1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_comments.head()\n",
    "data_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c2a1397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\kedar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'Didn' is likely misspelled. Suggestions: ['did', 'dian', 'din', 'didna', 'didnt']\n",
      "location: \n",
      "location: \n",
      "location: \n",
      "The word 'prices' is likely misspelled. Suggestions: ['price', 'priced', 'pricer', 'preces', 'pricks']\n",
      "location: \n",
      "location: \n",
      "location: \n",
      "location: \n",
      "The word 'Zealand' is likely misspelled. Suggestions: ['lealand']\n",
      "location: \n",
      "location: \n",
      "[{'location': Location(Did, Ilovik, Grad Mali Lošinj, Primorsko-goranska županija, 51552, Hrvatska, (44.4506199, 14.547613, 0.0))}, {'location': Location(Thüringen, Deutschland, (50.9014721, 11.0377839, 0.0))}, {'location': Location(Stop, Bovec, Slovenija, (46.4369086, 13.6728474, 0.0))}, {'location': Location(Price County, Wisconsin, United States, (45.676627, -90.368792, 0.0))}, {'location': Location(there, Uhembo, Central Alego ward, Alego Usonga, Siaya, Kenya, (0.13559495, 34.26772878953496, 0.0))}, {'location': Location(Campus Cut Though, Heslington, York, England, United Kingdom, (53.94709015, -1.0383845379306207, 0.0))}, {'location': Location(New, Owen County, Kentucky, United States, (38.4278468, -84.8105037, 0.0))}, {'location': Location(Lealand, Clarksville, Montgomery County, Middle Tennessee, Tennessee, 37042, United States, (36.5648113, -87.4090042366841, 0.0))}, {'location': Location(A, 120, Gale Place, Bronx County, The Bronx, City of New York, New York, 10463, United States, (40.8847691, -73.8933314, 0.0))}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'location': Location(Did, Ilovik, Grad Mali Lošinj, Primorsko-goranska županija, 51552, Hrvatska, (44.4506199, 14.547613, 0.0))},\n",
       " {'location': Location(Thüringen, Deutschland, (50.9014721, 11.0377839, 0.0))},\n",
       " {'location': Location(Stop, Bovec, Slovenija, (46.4369086, 13.6728474, 0.0))},\n",
       " {'location': Location(Price County, Wisconsin, United States, (45.676627, -90.368792, 0.0))},\n",
       " {'location': Location(there, Uhembo, Central Alego ward, Alego Usonga, Siaya, Kenya, (0.13559495, 34.26772878953496, 0.0))},\n",
       " {'location': Location(Campus Cut Though, Heslington, York, England, United Kingdom, (53.94709015, -1.0383845379306207, 0.0))},\n",
       " {'location': Location(New, Owen County, Kentucky, United States, (38.4278468, -84.8105037, 0.0))},\n",
       " {'location': Location(Lealand, Clarksville, Montgomery County, Middle Tennessee, Tennessee, 37042, United States, (36.5648113, -87.4090042366841, 0.0))},\n",
       " {'location': Location(A, 120, Gale Place, Bronx County, The Bronx, City of New York, New York, 10463, United States, (40.8847691, -73.8933314, 0.0))}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Download NLTK words corpus (if not already downloaded)\n",
    "nltk.download('words')\n",
    "\n",
    "def location_tagger(text):\n",
    "    geolocator = Nominatim(user_agent=\"my_geocoder\")\n",
    "    english_words = set(words.words())\n",
    "\n",
    "    text = text\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    detected_locations = []\n",
    "\n",
    "    # Iterate through tokens and check for potential locations\n",
    "    for token in tokens:\n",
    "        # Check if the token is a valid English word\n",
    "        if token.lower() not in english_words:\n",
    "            # Perform spell-checking using NLTK and suggest corrections\n",
    "            suggestions = [word for word in english_words if nltk.edit_distance(token.lower(), word) < 2]\n",
    "            print(f\"The word '{token}' is likely misspelled. Suggestions: {suggestions}\")\n",
    "\n",
    "            # Choose the suggestion with the smallest edit distance\n",
    "            best_suggestion = min(suggestions, key=lambda x: nltk.edit_distance(token.lower(), x))\n",
    "            location = geolocator.geocode(best_suggestion)\n",
    "        else:\n",
    "            # Perform geocoding to get the location information\n",
    "            location = geolocator.geocode(token)\n",
    "\n",
    "        # Check if a location was found\n",
    "        if location:\n",
    "            print(\"location: \")\n",
    "            detected_locations.append({'location': location})\n",
    "        else:\n",
    "            location = 'unknown location'\n",
    "            detected_locations.append({'location': location})\n",
    "\n",
    "    print(detected_locations)\n",
    "    return detected_locations\n",
    "\n",
    "# Example usage\n",
    "location_tagger(\"Didn't stop prices there though. New Zealand a...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95c16514",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\kedar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'Didn' is likely misspelled. Suggestions: ['did', 'dian', 'din', 'didna', 'didnt']\n",
      "Input: Didn\n",
      "Countries: \n",
      "Regions: \n",
      "Cities: \n",
      "Result: \n",
      "Input: t\n",
      "Countries: \n",
      "Regions: \n",
      "Cities: \n",
      "Result: \n",
      "Input: stop\n",
      "Countries: \n",
      "Regions: \n",
      "Cities: \n",
      "Result: \n",
      "The word 'prices' is likely misspelled. Suggestions: ['price', 'priced', 'pricer', 'preces', 'pricks']\n",
      "Input: prices\n",
      "Countries: \n",
      "Regions: \n",
      "Cities: \n",
      "Result: \n",
      "Input: there\n",
      "Countries: \n",
      "Regions: \n",
      "Cities: \n",
      "Result: \n",
      "Input: though\n",
      "Countries: \n",
      "Regions: \n",
      "Cities: \n",
      "Result: \n",
      "Input: pari\n",
      "Countries: \n",
      "Regions: \n",
      "Cities: \n",
      "Result: \n",
      "Input: a\n",
      "Countries: \n",
      "Regions: \n",
      "Cities: \n",
      "Result: \n",
      "[{'location': ''}, {'location': ''}, {'location': ''}, {'location': ''}, {'location': ''}, {'location': ''}, {'location': ''}, {'location': ''}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'location': ''},\n",
       " {'location': ''},\n",
       " {'location': ''},\n",
       " {'location': ''},\n",
       " {'location': ''},\n",
       " {'location': ''},\n",
       " {'location': ''},\n",
       " {'location': ''}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import locationtagger\n",
    "\n",
    "# Download NLTK words corpus (if not already downloaded)\n",
    "nltk.download('words')\n",
    "\n",
    "def location_tagger(text):\n",
    "    english_words = set(nltk.corpus.words.words())\n",
    "\n",
    "    text = text\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    detected_locations = []\n",
    "\n",
    "    # Iterate through tokens and check for potential locations\n",
    "    for token in tokens:\n",
    "        # Check if the token is a valid English word\n",
    "        if token.lower() not in english_words:\n",
    "            # Perform spell-checking using NLTK and suggest corrections\n",
    "            suggestions = [word for word in english_words if nltk.edit_distance(token.lower(), word) < 2]\n",
    "            print(f\"The word '{token}' is likely misspelled. Suggestions: {suggestions}\")\n",
    "\n",
    "            # Choose the suggestion with the smallest edit distance\n",
    "            best_suggestion = min(suggestions, key=lambda x: nltk.edit_distance(token.lower(), x))\n",
    "            place_entity = locationtagger.find_locations(text=best_suggestion)\n",
    "        else:\n",
    "            # Perform geocoding to get the location information\n",
    "            place_entity = locationtagger.find_locations(text=token)\n",
    "\n",
    "        # Extract countries, regions, and cities as strings\n",
    "        countries = ', '.join(place_entity.countries) if place_entity.countries else ''\n",
    "        regions = ', '.join(place_entity.regions) if place_entity.regions else ''\n",
    "        cities = ', '.join(place_entity.cities) if place_entity.cities else ''\n",
    "\n",
    "        # Combine countries, regions, and cities into a single string\n",
    "        result = f'{countries}, {regions}, {cities}'.strip(', ')\n",
    "\n",
    "        # Print statements for checking\n",
    "        print(f\"Input: {token}\")\n",
    "        print(f\"Countries: {countries}\")\n",
    "        print(f\"Regions: {regions}\")\n",
    "        print(f\"Cities: {cities}\")\n",
    "        print(f\"Result: {result}\")\n",
    "\n",
    "        detected_locations.append({'location': result})\n",
    "\n",
    "    print(detected_locations)\n",
    "    return detected_locations\n",
    "\n",
    "# Example usage\n",
    "location_tagger(\"Didn't stop prices there though. pari a...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6259a0f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68b120c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\kedar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Did\n",
      "Countries: \n",
      "Regions: \n",
      "Cities: \n",
      "Result: \n",
      "Input: n't\n",
      "Countries: \n",
      "Regions: \n",
      "Cities: \n",
      "Result: \n",
      "Input: stop\n",
      "Countries: \n",
      "Regions: \n",
      "Cities: \n",
      "Result: \n",
      "Input: prices\n",
      "Countries: \n",
      "Regions: \n",
      "Cities: \n",
      "Result: \n",
      "Input: there\n",
      "Countries: \n",
      "Regions: \n",
      "Cities: \n",
      "Result: \n",
      "Input: though\n",
      "Countries: \n",
      "Regions: \n",
      "Cities: \n",
      "Result: \n",
      "Input: .\n",
      "Countries: \n",
      "Regions: \n",
      "Cities: \n",
      "Result: \n",
      "Input: New\n",
      "Countries: \n",
      "Regions: \n",
      "Cities: \n",
      "Result: \n",
      "Input: Zealand\n",
      "Countries: \n",
      "Regions: \n",
      "Cities: \n",
      "Result: \n",
      "Input: a\n",
      "Countries: \n",
      "Regions: \n",
      "Cities: \n",
      "Result: \n",
      "Input: ...\n",
      "Countries: \n",
      "Regions: \n",
      "Cities: \n",
      "Result: \n",
      "[{'location': ''}, {'location': ''}, {'location': ''}, {'location': ''}, {'location': ''}, {'location': ''}, {'location': ''}, {'location': ''}, {'location': ''}, {'location': ''}, {'location': ''}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'location': ''},\n",
       " {'location': ''},\n",
       " {'location': ''},\n",
       " {'location': ''},\n",
       " {'location': ''},\n",
       " {'location': ''},\n",
       " {'location': ''},\n",
       " {'location': ''},\n",
       " {'location': ''},\n",
       " {'location': ''},\n",
       " {'location': ''}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import locationtagger\n",
    "\n",
    "# Download NLTK words corpus (if not already downloaded)\n",
    "nltk.download('words')\n",
    "\n",
    "def location_tagger(text):\n",
    "    english_words = set(nltk.corpus.words.words())\n",
    "\n",
    "    # Tokenize the text using nltk.word_tokenize to preserve multi-word locations\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    detected_locations = []\n",
    "\n",
    "    i = 0  # Index for iterating through tokens\n",
    "    while i < len(tokens):\n",
    "        token = tokens[i]\n",
    "\n",
    "        # Check if the token is a valid English word\n",
    "        if token.lower() not in english_words:\n",
    "            # Combine consecutive tokens to form a potential multi-word location\n",
    "            multi_word_token = token\n",
    "            while i + 1 < len(tokens) and tokens[i + 1].lower() not in english_words:\n",
    "                i += 1\n",
    "                multi_word_token += f' {tokens[i]}'\n",
    "\n",
    "            # Perform spell-checking using NLTK and suggest corrections\n",
    "            suggestions = [word for word in english_words if nltk.edit_distance(multi_word_token.lower(), word) < 2]\n",
    "            \n",
    "            # Check if there are suggestions before finding the best one\n",
    "            if suggestions:\n",
    "                # Choose the suggestion with the smallest edit distance\n",
    "                best_suggestion = min(suggestions, key=lambda x: nltk.edit_distance(multi_word_token.lower(), x))\n",
    "                place_entity = locationtagger.find_locations(text=best_suggestion)\n",
    "            else:\n",
    "                place_entity = locationtagger.find_locations(text=multi_word_token)\n",
    "        else:\n",
    "            # Perform geocoding to get the location information\n",
    "            place_entity = locationtagger.find_locations(text=token)\n",
    "\n",
    "        # Extract countries, regions, and cities as strings\n",
    "        countries = ', '.join(place_entity.countries) if place_entity.countries else ''\n",
    "        regions = ', '.join(place_entity.regions) if place_entity.regions else ''\n",
    "        cities = ', '.join(place_entity.cities) if place_entity.cities else ''\n",
    "\n",
    "        # Combine countries, regions, and cities into a single string\n",
    "        result = f'{countries}, {regions}, {cities}'.strip(', ')\n",
    "\n",
    "        # Print statements for checking\n",
    "        print(f\"Input: {token}\")\n",
    "        print(f\"Countries: {countries}\")\n",
    "        print(f\"Regions: {regions}\")\n",
    "        print(f\"Cities: {cities}\")\n",
    "        print(f\"Result: {result}\")\n",
    "\n",
    "        detected_locations.append({'location': result})\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    print(detected_locations)\n",
    "    return detected_locations\n",
    "\n",
    "# Example usage\n",
    "location_tagger(\"Didn't stop prices there though. New Zealand a...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa7e2fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\kedar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Didn\n",
      "Countries: \n",
      "Regions: \n",
      "Cities: \n",
      "Result: \n",
      "Input: t\n",
      "Countries: \n",
      "Regions: \n",
      "Cities: \n",
      "Result: \n",
      "Input: stop\n",
      "Countries: \n",
      "Regions: \n",
      "Cities: \n",
      "Result: \n",
      "Input: prices\n",
      "Countries: \n",
      "Regions: \n",
      "Cities: \n",
      "Result: \n",
      "Input: there\n",
      "Countries: \n",
      "Regions: \n",
      "Cities: \n",
      "Result: \n",
      "Input: though\n",
      "Countries: \n",
      "Regions: \n",
      "Cities: \n",
      "Result: \n",
      "Input: pari\n",
      "Countries: \n",
      "Regions: \n",
      "Cities: \n",
      "Result: \n",
      "Input: a\n",
      "Countries: \n",
      "Regions: \n",
      "Cities: \n",
      "Result: \n",
      "[{'location': ''}, {'location': ''}, {'location': ''}, {'location': ''}, {'location': ''}, {'location': ''}, {'location': ''}, {'location': ''}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'location': ''},\n",
       " {'location': ''},\n",
       " {'location': ''},\n",
       " {'location': ''},\n",
       " {'location': ''},\n",
       " {'location': ''},\n",
       " {'location': ''},\n",
       " {'location': ''}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import locationtagger\n",
    "\n",
    "# Download NLTK words corpus (if not already downloaded)\n",
    "nltk.download('words')\n",
    "\n",
    "def location_tagger(text):\n",
    "    english_words = set(nltk.corpus.words.words())\n",
    "\n",
    "    text = text\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    detected_locations = []\n",
    "\n",
    "    # Iterate through tokens and check for potential locations\n",
    "    for token in tokens:\n",
    "        # Check if the token is a valid English word\n",
    "        if token.lower() not in english_words:\n",
    "            # Perform spell-checking using NLTK and suggest corrections\n",
    "            suggestions = [word for word in english_words if nltk.edit_distance(token.lower(), word) < 2]\n",
    "\n",
    "            if suggestions:\n",
    "                # Choose the suggestion with the smallest edit distance\n",
    "                best_suggestion = min(suggestions, key=lambda x: nltk.edit_distance(token.lower(), x))\n",
    "                place_entity = locationtagger.find_locations(text=best_suggestion)\n",
    "            else:\n",
    "                # If no suggestions, use the original token for location tagging\n",
    "                place_entity = locationtagger.find_locations(text=token)\n",
    "        else:\n",
    "            # Perform geocoding to get the location information\n",
    "            place_entity = locationtagger.find_locations(text=token)\n",
    "\n",
    "        # Extract countries, regions, and cities as strings\n",
    "        countries = ', '.join(place_entity.countries) if place_entity.countries else ''\n",
    "        regions = ', '.join(place_entity.regions) if place_entity.regions else ''\n",
    "        cities = ', '.join(place_entity.cities) if place_entity.cities else ''\n",
    "\n",
    "        # Combine countries, regions, and cities into a single string\n",
    "        result = f'{countries}, {regions}, {cities}'.strip(', ')\n",
    "\n",
    "        # Print statements for checking\n",
    "        print(f\"Input: {token}\")\n",
    "        print(f\"Countries: {countries}\")\n",
    "        print(f\"Regions: {regions}\")\n",
    "        print(f\"Cities: {cities}\")\n",
    "        print(f\"Result: {result}\")\n",
    "\n",
    "        detected_locations.append({'location': result})\n",
    "\n",
    "    print(detected_locations)\n",
    "    return detected_locations\n",
    "\n",
    "# Example usage\n",
    "location_tagger(\"Didn't stop prices there though. pari a...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d034c18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspellchecker\n",
      "  Downloading pyspellchecker-0.7.2-py3-none-any.whl (3.4 MB)\n",
      "     ---------------------------------------- 3.4/3.4 MB 2.1 MB/s eta 0:00:00\n",
      "Installing collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.7.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\kedar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\kedar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\kedar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\kedar\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "DEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg<=0.29.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: C:\\Users\\kedar\\AppData\\Local\\programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspellchecker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d1fe8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: in\n",
      "Countries: \n",
      "Regions: \n",
      "Cities: \n",
      "Result: \n",
      "Input: the\n",
      "Countries: \n",
      "Regions: \n",
      "Cities: \n",
      "Result: \n",
      "Input: city\n",
      "Countries: \n",
      "Regions: \n",
      "Cities: \n",
      "Result: \n",
      "Input: of\n",
      "Countries: \n",
      "Regions: \n",
      "Cities: \n",
      "Result: \n",
      "Input: paris\n",
      "Countries: \n",
      "Regions: Paris\n",
      "Cities: Paris\n",
      "Result: Paris, Paris\n",
      "[{'location': ''}, {'location': ''}, {'location': ''}, {'location': ''}, {'location': 'Paris, Paris'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'location': ''},\n",
       " {'location': ''},\n",
       " {'location': ''},\n",
       " {'location': ''},\n",
       " {'location': 'Paris, Paris'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "import locationtagger\n",
    "\n",
    "def location_tagger(text):\n",
    "    spell = SpellChecker()\n",
    "    text = text\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    detected_locations = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if not spell.correction(token).lower() == token.lower():\n",
    "            corrected_token = spell.correction(token)\n",
    "            place_entity = locationtagger.find_locations(text=corrected_token)\n",
    "        else:\n",
    "            place_entity = locationtagger.find_locations(text=token)\n",
    "\n",
    "        # Extract countries, regions, and cities as strings\n",
    "        countries = ', '.join(place_entity.countries) if place_entity.countries else ''\n",
    "        regions = ', '.join(place_entity.regions) if place_entity.regions else ''\n",
    "        cities = ', '.join(place_entity.cities) if place_entity.cities else ''\n",
    "\n",
    "        # Combine countries, regions, and cities into a single string\n",
    "        result = f'{countries}, {regions}, {cities}'.strip(', ')\n",
    "\n",
    "        # Print statements for checking\n",
    "        print(f\"Input: {token}\")\n",
    "        print(f\"Countries: {countries}\")\n",
    "        print(f\"Regions: {regions}\")\n",
    "        print(f\"Cities: {cities}\")\n",
    "        print(f\"Result: {result}\")\n",
    "\n",
    "        detected_locations.append({'location': result})\n",
    "\n",
    "    print(detected_locations)\n",
    "    return detected_locations\n",
    "\n",
    "# Example usage\n",
    "location_tagger(\"in the city of paris\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
